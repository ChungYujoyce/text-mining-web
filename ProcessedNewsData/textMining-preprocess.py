# -*- coding: utf-8 -*-
"""TextMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10AKte-Z7OZpv6UmKBtPZo3uCSs7EMDlg
"""
import sys
if len(sys.argv) != 2:
    print("Number of arguments is not correct !!")
    print("Usage: python3 textMining-preprocess.py /path/to/news.txt")
    sys.exit()

TEXT = sys.argv[1]

import nltk
text = open(TEXT).read().lower()

###tokanization
nltk.download('punkt')
tokens = nltk.word_tokenize(text)
# print(tokens)

###use stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk_stopwords = stopwords.words('english')
#print('ntlk stopwords:',nltk_stopwords)
user_stopwords =["the","and","i"]
stopwords = user_stopwords + nltk_stopwords
# print("ours built stopwords", stopwords)

tokens_f = [term for term in tokens if term not in stopwords and term.isalnum()]
# print(tokens_f)
# print(len(tokens) - len(tokens_f)) # 300

###stemming

from nltk import PorterStemmer
ps = PorterStemmer()
token_psstem = [ps.stem(word) for word in tokens_f]
# print(token_psstem)
###Lemmatization

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
token_lem = [wnl.lemmatize(word) for word in tokens_f]
# print(token_lem)

tokens_lemm=[]
for term in token_lem:
    v = wnl.lemmatize(term, pos="v")
    n = wnl.lemmatize(v, pos="n")
    a = wnl.lemmatize(n, pos="a")
    r = wnl.lemmatize(a, pos="r")
    tokens_lemm.append(r)
# print(tokens_lemm)

tokens_freq=nltk.FreqDist(tokens_lemm)
# print("tokens top 10 frequency word",tokens_freq.most_common(10))

new_file_name = TEXT.split(".")[0]
f = open("processed-" + new_file_name + ".txt", mode = 'w')

f.write(' '.join(tokens_lemm))
f.close()
